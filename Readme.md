# Reinforcement Learning Algorithms implementation
This archive contains 3 reinforcement learning algorithms implemented in Python and PyTorch, their components
and utils needed to run and test them. There are 3 folders:

- DeepQLearning -- Contains DQN, DDQN and DuelingPDQN agents, neural networks, memories and main file to run the training.
- Utils -- Contains the utils that are used to monitor agent's performance, plot results, generate charts and test agents.
- FinalData -- Contains all the data generated by the agents during training and testing. The data was used in the text.

Changing the folder structure might result in errors when running the code -- the paths to save models, reports, charts etc.
are set to work with the current structure.

## Dependencies
The project was developed, used and tested with **Python 3.10** and **PyTorch 2.1.1+cu121** (the CUDA version). The other dependencies are:

- gymnasium
- numpy
- argparse
- dataclasses
- pandas
- matplotlib
- seaborn
- colorama

## Running the training
The QMain.py file in the DeepQLearning folder is the main file to run the training of the agents. There are up to
24 arguments that can be passed to the script to tweak the hyperparameters; but no need to worry, the absolute majority
of them have **default values that are set to work well with the environments** -- usually, only algorithm and env_name need
to be changed. The hyperparameters are described in QMain.py or in constructors of the agents.

For convenience, these are the arguments that were used to run the experiments in the text:

- `-env_name ALE/Pong-v5 -save_name Pong-episode- -algorithm ALGNAME -training_episodes 300`
- `-env_name ALE/Boxing-v5 -save_name Boxing-episode- -algorithm ALGNAME -training_episodes 800 -epsilon_decay 0.0000225 -tau 0.001`

The ALGNAME needs to be replaced by either DQN (Deep Q-Learning), DDQN (Double Deep Q-Learning) or DuelingPDQN (Dueling Deep Q-Learning with Prioritized Experience Replay).
If you do not want to draw charts during training, you can add the **-draw_charts False** argument. The training results 
are saved in DeepQLearning/Data/ALGNAME (.csv) and DeepQLearning/Models/ALGNAME (.pth) folders.

## Running the testing
The GameRunner.py file is used to test the agents; it **loads the model from the arguments** and runs the agent in the environment.
The arguments are, again, described in the file. 

For convenience, these are the arguments that are used to show the performance of all the agents:

- `-algorithm DuelingPDQN -env_name ALE/Boxing-v5 -network_path ../FinalData/DuelingPDQN/Boxing-episode-Max.pth -mode show`
- `-algorithm DQN -env_name ALE/Boxing-v5 -network_path ../FinalData/DQN/Boxing-episode-Max.pth -mode show`
- `-algorithm DDQN -env_name ALE/Boxing-v5 -network_path ../FinalData/DDQN/Boxing-episode-Max.pth -mode show`
- `-algorithm DuelingPDQN -env_name ALE/Pong-v5 -network_path ../FinalData/DuelingPDQN/Pong-episode-Max.pth -mode show`
- `-algorithm DQN -env_name ALE/Pong-v5 -network_path ../FinalData/DQN/Pong-episode-Max.pth -mode show`
- `-algorithm DDQN -env_name ALE/Pong-v5 -network_path ../FinalData/DDQN/Pong-episode-Max.pth -mode show`

If you want to test the performance of the agents on multiple episodes, you can change the **-mode** argument to **calculate** and
the **-episodes** argument to the **number of testing episodes**. The results will be saved in the FinalData folder, overwriting the current ones.

## Additional info
In FinalData folder, there are 2 models for each agent and environment; the one that achieved the highest average reward
(**game-episode-Max.pth**) and the one that was saved in the last episode of training (**game-episode-300** for Pong and **game-episode-800** for Boxing).
For evaluation, I always used the Max model.

## Settings, fixes
Reinforcement learning can be very unpredictable and issues sometimes occur; these are the ones I encountered and how I fixed them:

- **CUDA out of memory** --  Decrease the batch size; the default value is 128, which worked quite well for me (RTX 4070), but the commonly
used value is lower (ranging from 32 to 64, even 16 is sometimes used).
- **DDQN is making very little progress** -- DDQN is much more sensitive to tau hyperparameter because it uses both networks
during its update. That (combined with soft tau update) might sometimes lead to issues. The tau value I used is mainly "tuned" for
the DQN update, so **lowering the tau** (values can range from 0.001 to even 0.000x) might do the trick.
- **DuelingPDQN is making very little progress** -- At first few episodes, that is normal; if the issue persists, try tweaking the **alpha** hyperparameter.